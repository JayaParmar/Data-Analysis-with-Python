import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from matplotlib import pyplot as plt

def coefficient_of_determination():
    df=pd.read_csv("mystery_data.tsv",sep='\t')

    x1=df["X1"]
    x2=df['X2']
    x3=df['X3']
    x4=df['X4']
    x5=df['X5']
    y=df['Y']

    model=LinearRegression(fit_intercept=False)
    model.fit(np.vstack([x1,x2,x3,x4,x5]).T, y)
    xfit=np.linspace(-50,150, 1000)
    yfit=model.predict(np.vstack([x1,x2,x3,x4,x5]).T)
    #plt.plot(xfit,yfit)
    #plt.plot(x1,x2,x3,x4,x5,y,'o')
    print("R2-score with feature(s) X:",r2_score(y,model.predict(np.vstack([x1,x2,x3,x4,x5]).T)))

    
    model1=LinearRegression(fit_intercept=False)
    model1.fit(np.vstack([x1]).T, y)
    x1fit=np.linspace(-50,150, 1000)
    y1fit=model1.predict(np.vstack([x1]).T)
    #plt.plot(x1fit,y1fit)
    print("R2-score with feature(s) X1:",r2_score(y,model1.predict(np.vstack([x1]).T)))

    model2=LinearRegression(fit_intercept=False)
    model2.fit(np.vstack([x2]).T, y)
    x2fit=np.linspace(-50,150, 1000)
    y2fit=model2.predict(np.vstack([x2]).T)
    #plt.plot(x2fit,y2fit,label='predicted_values')
    print("R2-score with feature(s) X2:",r2_score(y,model2.predict(np.vstack([x2]).T)))
    
    model3=LinearRegression(fit_intercept=False)
    model3.fit(np.vstack([x3]).T, y)
    x3fit=np.linspace(-50,150, 1000)
    y3fit=model3.predict(np.vstack([x3]).T)
    #plt.plot(x3fit,y3fit)
    print("R2-score with feature(s) X3:",r2_score(y,model3.predict(np.vstack([x3]).T)))
    
    model4=LinearRegression(fit_intercept=False)
    model4.fit(np.vstack([x4]).T, y)
    x4fit=np.linspace(-50,150, 1000)
    y4fit=model4.predict(np.vstack([x4]).T)
    #plt.plot(x4fit,y4fit)
    print("R2-score with feature(s) X4:",r2_score(y,model4.predict(np.vstack([x4]).T)))
    
    model5=LinearRegression(fit_intercept=False)
    model5.fit(np.vstack([x4]).T, y)
    x5fit=np.linspace(-50,150, 1000)
    y5fit=model5.predict(np.vstack([x5]).T)
    #plt.plot(x5fit,y5fit)
    print("R2-score with feature(s) X5:",r2_score(y,model5.predict(np.vstack([x5]).T)))
    
coefficient_of_determination()
_______________________________________________________________________________________________________________________________________
Output:
#With fit_intercept = False
R2-score with feature(s) X: 1.0
R2-score with feature(s) X1: -0.2912746022850772
R2-score with feature(s) X2: -0.11539459114400685
R2-score with feature(s) X3: -0.39881489753744437
R2-score with feature(s) X4: -0.2003341232845286
R2-score with feature(s) X5: 0.503907346776044

#With fit_intercept = True
R2-score with feature(s) X: 1.0
R2-score with feature(s) X1: 0.016918272602768458
R2-score with feature(s) X2: 0.008964585308688822
R2-score with feature(s) X3: 0.08785404530656271
R2-score with feature(s) X4: 0.00030237088254569944
R2-score with feature(s) X5: -0.03321556211322929

#Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.
